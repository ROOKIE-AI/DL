# 深度学习 学习笔记



## 目录

- [简介](#简介)

- [安装指南](#安装指南)

- [笔记结构](#笔记结构)

- [学习资源](#学习资源)

- [进度记录](#进度记录)

  

## 简介

​	欢迎来到我的机器学习学习笔记仓库！本仓库主要记录我在学习机器学习过程中所整理的笔记、代码和相关资源。希望这些内容对你也有所帮助。



## 安装指南

​	如果想运行本仓库中的代码，请按照以下步骤进行设置：

1. **克隆仓库**：
   
   ```bash
   git clone https://github.com/ROOKIE-AI/DL.git
   ```



## 笔记结构



## 学习资源

​	学习过程中使用的一些资源

### 课程

* **[Andrew Ng:Deep Learning Specialization](https://www.bilibili.com/video/BV1FT4y1E74V?vd_source=b6653eb93cde9931ca6d7c2760d15b2d)**
* **[李宏毅:机器学习与深度学习](https://www.bilibili.com/video/BV1J94y1f7u5?vd_source=b6653eb93cde9931ca6d7c2760d15b2d)**

* **[fast.ai:面向程序员的实用深度学习](https://course.fast.ai/)**
* **[李沐动手学习深度学习](https://zh.d2l.ai/)**
* **[google:深度学习Tensorflow简介](https://learn.udacity.com/courses/ud187)**


### 书籍

### 其他
* [可视化卷积神经网络](https://ai-demos.cocorobo.hk/cnn-explainer/public/)
* [深度学习调整手册](https://developers.google.com/machine-learning/guides/deep-learning-tuning-playbook?hl=zh-cn)

## 进度记录



### 第一周

#### 2024-06-22 星期六
* 学习卷积神经网络：观看李宏毅课程, 同时阅读了李沐的动手学习深度学习书籍卷积神经网络章节部分。
* 几个重要收获:
  1. 卷积神经网络通过局部分析+共享参数， 结合图像的一些基本特性， 降低了模型的弹性，但提升了模型在图像任务上的表示能力
  1. 卷积神经网络同时应用于当年赫赫有名的AlphaGo， 但它没有使用池化层， 因为在围棋比赛中比较重视细节，通过池化层汇聚往往使其无法很好地表现。(对为什么图像处理可以用池化层的原因解释)
  1. 通过多个卷积层后， 卷积核可以看到更大的区域
  1. 卷积神经网络最早期发明是应用于数字识别， 称为LeNet，2012年的AlexNet在此基础上又加入了几个卷积层， 同时选用ReLU激活函数代替原来的Sigmod

#### 2024-06-23 星期日

* 学习内容:  Kaggle 计算机视觉 课程，构建图像分类器和了解卷积的特征提取过程
* 重要收获：
  * 关于迁移学习: 冻结原有模型的参数， 并在其原输出上加入几个全连接层， 就可以实现二分类判别卡车和小汽车
  * 关于过拟合：模型在训练集上的损失函数随epoch持续下降， 但在验证集上却发生了上升， 是过拟合的表现
  * 关于激活函数:  对于二分类概率预测， 最后输出通常使用sigmoid 函数
  * 关于损失函数:  在二分类任务上， 常使用二元交叉熵损失函数
  * 卷积分类器用于特征提取的前两个操作：使用 **卷积** **过滤** 图像并使用 **ReLU** **检测**特征



### 第二周

#### 2024-06-24 星期一

* 学习内容: Kaggle 计算机视觉课程，学习池化、滑动窗口(填充和步幅)、自定义卷积神经网络和数据增强
* 重要收获:
  * 关于最大池化: 使网络具备了平移不变性， 同样的能力在卷积的共享参数中也有
  * 关于步幅：步幅过大， 会使信息损失较多，通常在较大卷积核上使用较大的步幅
  * 数据增强: 数据增强可以增加数据量， 一定程度上防止过拟合

#### 2024-06-25 星期二

* 学习内容: 李沐动手学习机器学习

  * 11 模型选择 + 过拟合和欠拟合
    * 模型容量和数据量与欠拟合过拟合之间的关系
    * 关于训练集、验证集和测试集的划分
    * 手写实现欠拟合、过拟合的多项式拟合线性回归
  * 12 权重衰退
    * 通过加入正则项，由权重衰退来限制模型的容量， 使其拟合过程中更加平滑， 防止其过拟合

* 重要收获：

  * 验证集的作用：主要用于超参数的调整， 在数据量比较小的情况下， 可以采用K折交叉验证。 

    * K折交叉验证的划分选取， 通常K 取 5 或者 10， 最终的模型选取方案有很多， 比如
      1. 从K 个模型中选取损失最小的
      2. 直接将K个模型用作推理， 最终取K个模型推理的均值(回归)或投票结果(分类) （集成学习)
      3. 将超参数在整个训练集和验证集上再重新训练一遍
    * 对于样本不均衡情况(比如正例样本远少于反例样本)，测试集尽量选取均衡

  * 为什么会过拟合?

    * 模型容量太大， 或者说弹性太大， 拟合能力太强， 在训练集上学习到了过多的特征， 从而在测试集及验证集上表现反而随着训练迭代次数的增加而变差

  * 如何有效防止过拟合？

    * 增加样本量: 例如图像中可以通过简单的反转等操作增加样本集数量
    * 进行正则化: 例如在损失函数上加入惩罚项，限制模型容量，防止模型学到比较大的参数， 使拟合函数更加平滑一些

  * 如何判断模型发生过拟合了?

    * 随着迭代次数的增加， 模型在训练集上的loss 在不断下降， 但是在验证集上反而开始上升
    * .... -> 这个问题还需要进一步深入探讨  ★★★

  * 关于为什么CNN在图像识别上如此有效

    * 从函数拟合能力的角度上来看， 单个隐藏层的MLP足以拟合任何形态的函数， 但通常单隐藏层的MLP却学不到一些复杂的东西。 事实上， 无论是CNN还是RNN， 它们展开其实本质上都是MLP, 但因为其考虑了很多数据的一些特性， 所以使其在具体任务上有更好的表现能力。 
    * CNN 通过 局部分析 和共享参数机制， 有效增强了模型对特征的表现能力

  * 为什么叫权重衰退?

    ​	可以对w进行求导， 写出梯度下降表达式， 可以写做: $\mathbf{w} \leftarrow \mathbf{w} - \eta \lambda \mathbf{w}$

* 遇到问题：

  * 手写实现多项式拟合，来分析过拟合， 但代码出现了问题， 多元线性拟合过程中， 梯度下降异常， 不确定是在哪个环节出现的问题， 待明天排查

#### 2024-06-26 星期三

* 学习内容:
  * 过拟合问题的正则化方案: 丢弃法(dropout)
  * PyTorch神经网络基础:  模型构建、模型自定义、自定义层、参数管理等
* 重要收获:
  * 丢弃法的辛顿解释
    * 可以认为丢弃法实质上是在训练阶段使用一个子神经网络做训练， 最终再由多个子神经网络集成来进行推理， 因此能够有效提升模型的准确性
  * 通常dropout 主要在全连接层上使用
  * PyTorch模型构建
    * 一般的神经网络可以直接通过nn来按顺序连接， 对于一些特殊的需要自定义的场景， 也可以通过继承nn.Module 父类来实现一个自定义的神经网络模型或者是layer或块
    * 神经网络中， 对于共用的一些层， 他们通常共享参数
  * dropout 随机丢弃， 如何保证其可重复性？
    * 可以通过固定随机数种子来实现， 但通常没有必要
  * cuda计算的浮点运算相加顺序会影响结果，通常cpu运算就不会出现这种情况
* 遇到问题:
  * 昨天的手写实现多项式拟合， 还未排查问题， 待明天
* 明日计划:
  1. 新课程安排: 李沐CNN课程部分19 - 22
  2. 动手实现环节: 多项式的线性拟合、正则化(丢弃法和权重衰退)

#### 2024-06-27 星期四

* 学习内容
  * 李沐动手学习深度学习： 卷积神经网络 结构、填充和步幅、多通道、池化
  * 多项式的线性拟合
* 重要收获
  * 卷积神经网络实质是一个特殊的MLP， 主要有两大特性， 分别是局部性和平移不变性， 局部性通过卷积核作用来实现， 平移不变性通过共用参数来实现
  * 卷积神经网络 为什么经常用 3 x 3 的， 而不是 更大的卷积核?
    * 李沐的说法是小的卷积核通常运算效率更高些， 但是3x3 的视野域经过3层卷积后才可以达到5x5卷积核一层的视野域， 但是3x3的推理3层后效率比5x5一层要低， 关于效率的考虑存疑
  * 池化Pooling 对 卷积神经网络的位置敏感性做了一定的钝化， 是实现平移不变性的一个重要特性
* 遇到问题：
  * 排查动手实现多项式线性拟合， 排查至最后环节， 发现甚至连基本的一元线性拟合都无法训练出来。 具体问题仍需明天排查
* 明日计划:
  * 动手实现环节:多项式的线性拟合问题排查， 验证实现正则化和丢弃法
  * 李沐动手学习深度学习课程： 经典卷积神经网络 LeNet、AlexNet 、VGG (23-25)

#### 2024-06-27 星期五

* 学习内容
  * 李沐动手学习深度学习: 卷积神经网络 LeNet 、AlexNet、VGG、NiN(23-26), 以及深度学习硬件(31)
  * 动手环节: 排查多项式线性拟合、运行实现LeNet(CPU， 并尝试调参优化， 准确性接近90%)和 AlexNet(GPU colab， 准确性达到90%)
* 重要收获
  * AlexNet 相比 LeNet 的改进主要在哪里？
    * 增加了卷积层， 同时使用了更多通道和更大的全连接层
    * 使用Relu作为激活函数
  * VGG 相比LeNet 的主要改进在哪里?
    * 使用可块的概念， 构造了VGG 块(多个重复卷积层 + 池化层)， 简化了神经网络的结构， 同时创建了更深的网络模块
    * 倾向使用更小的卷积核， 让模型做得更深， 相同计算开销情况下通常比做得更胖效果要好
  * NiN 相比 VGG 的主要改进是什么?
    * 通过 1x1卷积 + 池化 来代替原来的全连接， 并在整个模型的各个环节穿插使用， 最终再用一个全局卷积核将参数控制到通道数等于输出的节点数规模
    * 1x1 的卷积实现了同一像素不同通道之间的互相关运算， 可以认为是对不同像素点分别做了全连接， 它有效降低了之前使用全连接层时参数量
  * GPU 和 CPU 在计算任务上的主要区别在哪里?
    * 用非常多的相对较弱的计算单元来计算， 会比使用不多的cpu 更快
    * GPU 更适合大规模矩阵运算， 但是控制单元等相对会比较弱
  * 使用CPU做高性能计算的主要语言是什么?
    * 早期主要是Fortran， 因为它的编译器在高性能计算方面更好。但现在C++的编译器其实也很好了，所以目前C++在基于CPU的高性能计算方面更有优势
  * CPU提升计算任务效率的几个主要方面是什么?
    * 在内存优化方面主要有两个考虑方向
      1. 时间方面：数据重复调用， 会更大概率被cache
      2. 空间方面: 计算任务中尽量进行顺序计算。 例如, 对于大规模矩阵计算， 按行访问比按列更快
    * 进行多CPU并行计算，或者HPC集群计算
  * 多项式线性拟合 代码中出现的问题
    * 模拟数据的labels的shape 忘记设置为 (-1， 1)了， 导致最终计算的损失loss有问题。多敲代码， 注意检查!!!、
* 遇到问题:
  * 使用多项式拟合 进行过拟合分析时， 效果并不明显， 感觉可能和高幂项计算的数值比较小， 导致loss被椭化有关系(值得后续分析的问题， 最近暂时不作深究) ★★★
* 明日计划:
  * 动手学习深度学习课程：GoogleNet、批量归一化、残差网络 (27 - 29)
  * 动手环节: 能训练哪个就动哪个，取决于GPU

#### 2024-06-28 星期六

* 学习内容:
  * 动手学习深度学习课程: GoogleNet 、批量归一化 和 残差网络， 额外学习了 序列模型(50)
  * 动手环节: 训练VGG， 重新设计实现VGG网络模块
* 重要收获：
  * GoogleNet 相比于之前的NiN、VGG 等架构的主要创新是什么?
    * 加入了并行的不同卷积结构通道， 最后进行汇总
  * 批量归一化的主要目的是什么?
    * 能够有效加速梯度下降， 但是对于精度提升基本帮助不大
  * 残差网络的主要创新是什么?
    * 加入了残差块， 使得模型能够更好地保留低层学到的特征
  * 序列模型预测
    * 基于条件概率的马尔可夫假设， 实现对文本的预测。这个可以用MLP来实现， 但是在多步转移过程中误差很大， 一个比较好的方式就是采用潜变量， 来表示历史的信息。
* 明日计划:
  * 休息一天

#### 第二周总结

* 主要学习内容

  > 主要围绕计算机视觉CV进行学习，除此之外掌握了一些常用的防止过拟合技术以及数据集划分方式，对PyTorch 实践， 还有硬件相关的一些知识

  * 理论环节
    * 关于深度学习框架: 学习掌握设计自定义及使用默认模块的神经网络、参数管理、模型存储等PyTorch基础
    * 关于训练集的划分: 了解学习 K折交叉验证等常用划分手段， 并理解其主要划分标准原因
    * 关于模型过拟合问题: 学习两种主要的正则化技术: 权重衰退和丢弃法， 同时通过数据增强来控制模型过拟合等
    * 关于卷积神经网络的基础概念: 卷积网络基本结构、卷积互相关计算、填充、步幅、池化(汇聚)、多通道输入和输出等
    * 关于经典的卷积神经网络模型: LeNet、AlexNet、VGG、NiN、GoogleNet、ResNet
    * 关于硬件知识: CPU 和 GPU 的硬件结构介绍和主要的优缺点对比等
  * 实践环节
    * 从零动手实现多项式拟合线性网络的线性回归模型训练， 用于分析过拟合问题
    * 动手实践PyTorch神经网络设计
    * 动手实践卷积神经网络特征学习(水平边缘提取卷积核学习)
    * 动手实现LeNet 经典神经网络， 并在CPU上进行训练和调参优化
    * 动手实验AlexNet网络训练， 并在CoLab使用T4 GPU 进行训练和调参
    * 动手实验VGG模型训练(GPU), 并重新动手实现其网络的代码设计， 并进行调参分析
    * 动手实践在LeNet神经网络采用批量归一化
    * 动手实践在LeNet 神经网络上采用丢弃法， 有效降低了过拟合， 准确率直接达到91%附近
    * 动手实践Kaggle 计算机视觉课程, 掌握了解微调技术以及数据增强等技术， 同时尝试在Kaggle上进行训练和直观分析

* 下周主要计划:

  * 暂缓关于计算机视觉相关的模块学习， 开始自然语言处理NLP的入门， 关键任务是:
    * 掌握RNN系列的经典模型并进行从零实现， 主要掌握包括 RNN、LSTM、GRU等
    * 入门了解Transorform神经网络结构设计
    * 入门BERT， 简单了解其主要原理
  * 采用课程:
    * 仍以李沐的动手学习深度学习课程为主， 辅以李宏毅老师的课程部分
  * 侧重部分:
    * 重点了解掌握其理论部分， 暂时适当弱化动手环节， 待之后通过不断实践来进行强化

### 第三周

#### 2024-07-03 星期三

* 主要学习内容
  * 课程: 自然语言处理基础, 序列模型、文本预处理及语言模型数据集等
  * 动手实践:  分词、序列模型预测、制作语言模型数据集等
* 重要收获
  * 语言序列模型处理， 传统方式基于n阶马尔可夫进行简化假设
  * 通常有两种预测时间序列的方式， 一种叫做自回归模型， 还有一种是潜变量模型
* 存在问题
  * 很多， 李沐的课程和 书本读完后， 有比较多的疑问， 涉及了很多NLP模型的理论基础介绍，并做了部分介绍， 但是感觉有点乱
* 明日计划
  * 大致简单了解NLP的一些基本概念， 并快速入门RNN循环神经网络
  * 动手实现循环神经网络

#### 2024-07-04 星期四

* 主要学习内容
  * 课程: 系统了解RNN的神经网络结构等细节
  * 书籍: 系统学习了解RNN神经网络结构及常见变种， 加深理解。 了解其优化方法等
  * 动手实践： 逐行阅读李沐的RNN从零实现代码， 并运行
* 重要收获
  * RNN 的 工作流程及优化方法和训练过程
  * NLP 自然语言处理的分词等相关的基础知识
* 存在问题
  * 对RNN的从零实现训练环节代码解读未完成， 同时对于顺序批量化的加载数据训练出现异常， 待分析解决
  * 对RNN的损失函数选取(交叉熵损失函数)需要进一步了解
  * 对NLP传统处理手段的原理理解还不完整， 待后续加深
* 明日计划
  * 动手环节： 完整解读RNN从零实现代码， 掌握其PyTorch 的简洁实现版本
  * 课程环节： 学习 LSTM、GRU和深度循环神经网络以及双向循环神经网络

#### 2024-07-05 星期五

* 主要学习内容
  * 课程： 李沐动手学深度学习的 LSTM、GRU、深度循环神经网络和双向循环神经网络课程
  * 动手环节: 重新解读RNN实现代码， 并预测周杰伦歌词
* 重要收获
  * 了解和掌握了GRU、LSTM、深度循环神经网络及双向循环神经网络的基本原理
  * 进一步熟悉文字分词的过程， 加深印象
* 存在问题
  * 周杰伦歌词预测效果不佳， 曲线也一直在抖动， 待后续进一步调参分析
* 明日计划
  * 动手环节： 尽量实现一下LSTM等RNN循环神经网络， 主要以PyTorch 的形式
  * 课程: 基于李宏毅的课程学习自注意力机制的原理

#### 2024-07-06 星期六

* 主要学习内容
  * 课程: 李宏毅的自注意力机制课程等
* 重要收获
  * 对自注意力机制有了一定的了解
*  存在问题
  * 理解的不够深入
* 明日计划
  * 休息

#### 第三周总结

* 主要学习内容
  * 理论环节
    * 循环神经网络： RNN、LSTM、GRU、双向循环神经网络和深度循环神经网络
    * 自然语言基础：分词、one_hat编码、词嵌入等
    * 自注意力机制: 了解子注意力机制的实现过程和直觉性解释
  * 实践环节
    * 循环神经网络RNN的歌词预测从零实现
* 下周主要计划
  * 核心任务
    * 掌握Transformer神经网络架构
    * 入门和微调BERT、GPT 模型
  * 次要任务
    * RNN实践环节， 翻译、seq2seq等
    * PyTorch刘二大人课程完整学习， 系统掌握该框架
