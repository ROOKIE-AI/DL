{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标量的链式法则: <br><br>\n",
    "&emsp;&emsp; $y = f(u), u = g(x)$<br><br>\n",
    "&emsp;&emsp; $ \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial u} \\frac{\\partial u}{\\partial x} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拓展到向量:<br><br>\n",
    "&emsp;&emsp; $\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = \\frac{\\partial \\vec{y}}{\\partial \\vec{u}} \\frac{\\partial \\vec{u}}{\\partial \\vec{x}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 自动求导计算一个函数在指定值上的导数\n",
    "* 它有别于\n",
    "    * 符号求导(解析求导)\n",
    "* 数值求导:\n",
    "    * 通过数值拟合导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 将代码分解为操作子\n",
    "* 将计算表示成一个无环图\n",
    "* 显式构造\n",
    "    * Tensorflow/Theano/MXNet\n",
    "* 隐式构造:\n",
    "    * PyTorch/MXNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显式构造\n",
    "from mxnet import sym\n",
    "\n",
    "a = sym.var('a')\n",
    "b = sym.var('b')\n",
    "c = 2 * a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐式构造\n",
    "from mxnet import autograd, nd\n",
    "with autograd.record():\n",
    "    a = nd.ones((2, 1))\n",
    "    b = nd.ones((2, 1))\n",
    "    c = 2 * a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向累积 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求: $ y = 2x^Tx 的导数$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在计算y关于x的梯度之前, 我们需要一个地方来存储梯度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * x.T @ x\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 通过调用反向传播函数来自动计算y关于x每个分量的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;\n",
    "$\\frac{\\partial {y}}{\\partial {\\vec{x}}} =  4 \\vec{x} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad == 4 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 现在计算 x 的 另一个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认情况下, PyTorch 会积累梯度， 我们需要清除之前的值\n",
    "x.grad.zero_()\n",
    "y = x.sum()\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 深度学习中, 我们的目的不是计算微分矩阵, 而是批量中每个样本单独计算的偏导数之和 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自由练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ y = 2x_1^3 + 3x_1x_2 + 4x_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([1, 2])\n",
    "x.requires_grad_(True)\n",
    "y =  2*(x[0]**3) + 3*x[0]*x[1] + 4*x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.,  7.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(258., grad_fn=<AddBackward0>) 0 tensor([44., 16.])\n",
      "tensor(87.7600, grad_fn=<AddBackward0>) 1 tensor([26.4000,  3.2000])\n",
      "tensor(31.3888, grad_fn=<AddBackward0>) 2 tensor([15.8400,  0.6400])\n",
      "tensor(11.2918, grad_fn=<AddBackward0>) 3 tensor([9.5040, 0.1280])\n",
      "tensor(4.0647, grad_fn=<AddBackward0>) 4 tensor([5.7024, 0.0256])\n",
      "tensor(1.4633, grad_fn=<AddBackward0>) 5 tensor([3.4214, 0.0051])\n",
      "tensor(0.5268, grad_fn=<AddBackward0>) 6 tensor([2.0529e+00, 1.0240e-03])\n",
      "tensor(0.1896, grad_fn=<AddBackward0>) 7 tensor([1.2317e+00, 2.0480e-04])\n",
      "tensor(0.0683, grad_fn=<AddBackward0>) 8 tensor([7.3903e-01, 4.0960e-05])\n",
      "tensor(0.0246, grad_fn=<AddBackward0>) 9 tensor([4.4342e-01, 8.1920e-06])\n",
      "tensor(0.0088, grad_fn=<AddBackward0>) 10 tensor([2.6605e-01, 1.6384e-06])\n",
      "tensor(0.0032, grad_fn=<AddBackward0>) 11 tensor([1.5963e-01, 3.2768e-07])\n",
      "tensor(0.0011, grad_fn=<AddBackward0>) 12 tensor([9.5778e-02, 6.5536e-08])\n",
      "tensor(0.0004, grad_fn=<AddBackward0>) 13 tensor([5.7467e-02, 1.3107e-08])\n",
      "tensor(0.0001, grad_fn=<AddBackward0>) 14 tensor([3.4480e-02, 2.6214e-09])\n",
      "tensor(5.3500e-05, grad_fn=<AddBackward0>) 15 tensor([2.0688e-02, 5.2429e-10])\n",
      "tensor(1.9260e-05, grad_fn=<AddBackward0>) 16 tensor([1.2413e-02, 1.0486e-10])\n",
      "tensor(6.9336e-06, grad_fn=<AddBackward0>) 17 tensor([7.4477e-03, 2.0971e-11])\n",
      "tensor(2.4961e-06, grad_fn=<AddBackward0>) 18 tensor([4.4686e-03, 4.1943e-12])\n",
      "tensor(8.9859e-07, grad_fn=<AddBackward0>) 19 tensor([2.6812e-03, 8.3886e-13])\n",
      "tensor(3.2349e-07, grad_fn=<AddBackward0>) 20 tensor([1.6087e-03, 1.6777e-13])\n",
      "tensor(1.1646e-07, grad_fn=<AddBackward0>) 21 tensor([9.6523e-04, 3.3554e-14])\n",
      "tensor(4.1925e-08, grad_fn=<AddBackward0>) 22 tensor([5.7914e-04, 6.7109e-15])\n",
      "tensor(1.5093e-08, grad_fn=<AddBackward0>) 23 tensor([3.4748e-04, 1.3422e-15])\n",
      "tensor(5.4334e-09, grad_fn=<AddBackward0>) 24 tensor([2.0849e-04, 2.6844e-16])\n",
      "tensor(1.9560e-09, grad_fn=<AddBackward0>) 25 tensor([1.2509e-04, 5.3687e-17])\n",
      "tensor(7.0417e-10, grad_fn=<AddBackward0>) 26 tensor([7.5056e-05, 1.0737e-17])\n",
      "tensor(2.5350e-10, grad_fn=<AddBackward0>) 27 tensor([4.5034e-05, 2.1475e-18])\n",
      "tensor(9.1261e-11, grad_fn=<AddBackward0>) 28 tensor([2.7020e-05, 4.2950e-19])\n",
      "tensor(3.2854e-11, grad_fn=<AddBackward0>) 29 tensor([1.6212e-05, 8.5899e-20])\n",
      "tensor(1.1827e-11, grad_fn=<AddBackward0>) 30 tensor([9.7272e-06, 1.7180e-20])\n",
      "tensor(4.2579e-12, grad_fn=<AddBackward0>) 31 tensor([5.8363e-06, 3.4360e-21])\n",
      "tensor(1.5328e-12, grad_fn=<AddBackward0>) 32 tensor([3.5018e-06, 6.8719e-22])\n",
      "tensor(5.5182e-13, grad_fn=<AddBackward0>) 33 tensor([2.1011e-06, 1.3744e-22])\n",
      "tensor(1.9866e-13, grad_fn=<AddBackward0>) 34 tensor([1.2607e-06, 2.7488e-23])\n",
      "tensor(7.1516e-14, grad_fn=<AddBackward0>) 35 tensor([7.5639e-07, 5.4975e-24])\n",
      "tensor(2.5746e-14, grad_fn=<AddBackward0>) 36 tensor([4.5383e-07, 1.0995e-24])\n",
      "tensor(9.2685e-15, grad_fn=<AddBackward0>) 37 tensor([2.7230e-07, 2.1990e-25])\n",
      "tensor(3.3366e-15, grad_fn=<AddBackward0>) 38 tensor([1.6338e-07, 4.3980e-26])\n",
      "tensor(1.2012e-15, grad_fn=<AddBackward0>) 39 tensor([9.8028e-08, 8.7961e-27])\n",
      "tensor(4.3243e-16, grad_fn=<AddBackward0>) 40 tensor([5.8817e-08, 1.7592e-27])\n",
      "tensor(1.5567e-16, grad_fn=<AddBackward0>) 41 tensor([3.5290e-08, 3.5184e-28])\n",
      "tensor(5.6043e-17, grad_fn=<AddBackward0>) 42 tensor([2.1174e-08, 7.0369e-29])\n",
      "tensor(2.0175e-17, grad_fn=<AddBackward0>) 43 tensor([1.2704e-08, 1.4074e-29])\n",
      "tensor(7.2632e-18, grad_fn=<AddBackward0>) 44 tensor([7.6227e-09, 2.8147e-30])\n",
      "tensor(2.6147e-18, grad_fn=<AddBackward0>) 45 tensor([4.5736e-09, 5.6295e-31])\n",
      "tensor(9.4130e-19, grad_fn=<AddBackward0>) 46 tensor([2.7442e-09, 1.1259e-31])\n",
      "tensor(3.3887e-19, grad_fn=<AddBackward0>) 47 tensor([1.6465e-09, 2.2518e-32])\n",
      "tensor(1.2199e-19, grad_fn=<AddBackward0>) 48 tensor([9.8790e-10, 4.5036e-33])\n",
      "tensor(4.3917e-20, grad_fn=<AddBackward0>) 49 tensor([5.9274e-10, 9.0072e-34])\n",
      "tensor(1.5810e-20, grad_fn=<AddBackward0>) 50 tensor([3.5564e-10, 1.8014e-34])\n",
      "tensor(5.6917e-21, grad_fn=<AddBackward0>) 51 tensor([2.1339e-10, 3.6029e-35])\n",
      "tensor(2.0490e-21, grad_fn=<AddBackward0>) 52 tensor([1.2803e-10, 7.2057e-36])\n",
      "tensor(7.3765e-22, grad_fn=<AddBackward0>) 53 tensor([7.6819e-11, 1.4411e-36])\n",
      "tensor(2.6555e-22, grad_fn=<AddBackward0>) 54 tensor([4.6091e-11, 2.8823e-37])\n",
      "tensor(9.5599e-23, grad_fn=<AddBackward0>) 55 tensor([2.7655e-11, 5.7646e-38])\n",
      "tensor(3.4416e-23, grad_fn=<AddBackward0>) 56 tensor([1.6593e-11, 1.1529e-38])\n",
      "tensor(1.2390e-23, grad_fn=<AddBackward0>) 57 tensor([9.9557e-12, 2.3058e-39])\n",
      "tensor(4.4603e-24, grad_fn=<AddBackward0>) 58 tensor([5.9734e-12, 4.6116e-40])\n",
      "tensor(1.6057e-24, grad_fn=<AddBackward0>) 59 tensor([3.5841e-12, 9.2228e-41])\n",
      "tensor(5.7805e-25, grad_fn=<AddBackward0>) 60 tensor([2.1504e-12, 1.8441e-41])\n",
      "tensor(2.0810e-25, grad_fn=<AddBackward0>) 61 tensor([1.2903e-12, 3.6882e-42])\n",
      "tensor(7.4915e-26, grad_fn=<AddBackward0>) 62 tensor([7.7416e-13, 7.3989e-43])\n",
      "tensor(2.6969e-26, grad_fn=<AddBackward0>) 63 tensor([4.6450e-13, 1.4574e-43])\n",
      "tensor(9.7090e-27, grad_fn=<AddBackward0>) 64 tensor([2.7870e-13, 3.3631e-44])\n",
      "tensor(3.4952e-27, grad_fn=<AddBackward0>) 65 tensor([1.6722e-13, 1.1210e-44])\n",
      "tensor(1.2583e-27, grad_fn=<AddBackward0>) 66 tensor([1.0033e-13, 0.0000e+00])\n",
      "tensor(4.5298e-28, grad_fn=<AddBackward0>) 67 tensor([6.0199e-14, 0.0000e+00])\n",
      "tensor(1.6307e-28, grad_fn=<AddBackward0>) 68 tensor([3.6119e-14, 0.0000e+00])\n",
      "tensor(5.8707e-29, grad_fn=<AddBackward0>) 69 tensor([2.1671e-14, 0.0000e+00])\n",
      "tensor(2.1134e-29, grad_fn=<AddBackward0>) 70 tensor([1.3003e-14, 0.0000e+00])\n",
      "tensor(7.6084e-30, grad_fn=<AddBackward0>) 71 tensor([7.8017e-15, 0.0000e+00])\n",
      "tensor(2.7390e-30, grad_fn=<AddBackward0>) 72 tensor([4.6810e-15, 0.0000e+00])\n",
      "tensor(9.8605e-31, grad_fn=<AddBackward0>) 73 tensor([2.8086e-15, 0.0000e+00])\n",
      "tensor(3.5498e-31, grad_fn=<AddBackward0>) 74 tensor([1.6852e-15, 0.0000e+00])\n",
      "tensor(1.2779e-31, grad_fn=<AddBackward0>) 75 tensor([1.0111e-15, 0.0000e+00])\n",
      "tensor(4.6005e-32, grad_fn=<AddBackward0>) 76 tensor([6.0666e-16, 0.0000e+00])\n",
      "tensor(1.6562e-32, grad_fn=<AddBackward0>) 77 tensor([3.6400e-16, 0.0000e+00])\n",
      "tensor(5.9622e-33, grad_fn=<AddBackward0>) 78 tensor([2.1840e-16, 0.0000e+00])\n",
      "tensor(2.1464e-33, grad_fn=<AddBackward0>) 79 tensor([1.3104e-16, 0.0000e+00])\n",
      "tensor(7.7271e-34, grad_fn=<AddBackward0>) 80 tensor([7.8623e-17, 0.0000e+00])\n",
      "tensor(2.7817e-34, grad_fn=<AddBackward0>) 81 tensor([4.7174e-17, 0.0000e+00])\n",
      "tensor(1.0014e-34, grad_fn=<AddBackward0>) 82 tensor([2.8304e-17, 0.0000e+00])\n",
      "tensor(3.6051e-35, grad_fn=<AddBackward0>) 83 tensor([1.6983e-17, 0.0000e+00])\n",
      "tensor(1.2979e-35, grad_fn=<AddBackward0>) 84 tensor([1.0190e-17, 0.0000e+00])\n",
      "tensor(4.6723e-36, grad_fn=<AddBackward0>) 85 tensor([6.1138e-18, 0.0000e+00])\n",
      "tensor(1.6820e-36, grad_fn=<AddBackward0>) 86 tensor([3.6683e-18, 0.0000e+00])\n",
      "tensor(6.0552e-37, grad_fn=<AddBackward0>) 87 tensor([2.2010e-18, 0.0000e+00])\n",
      "tensor(2.1799e-37, grad_fn=<AddBackward0>) 88 tensor([1.3206e-18, 0.0000e+00])\n",
      "tensor(7.8476e-38, grad_fn=<AddBackward0>) 89 tensor([7.9234e-19, 0.0000e+00])\n",
      "tensor(2.8251e-38, grad_fn=<AddBackward0>) 90 tensor([4.7541e-19, 0.0000e+00])\n",
      "tensor(1.0170e-38, grad_fn=<AddBackward0>) 91 tensor([2.8524e-19, 0.0000e+00])\n",
      "tensor(3.6614e-39, grad_fn=<AddBackward0>) 92 tensor([1.7115e-19, 0.0000e+00])\n",
      "tensor(1.3181e-39, grad_fn=<AddBackward0>) 93 tensor([1.0269e-19, 0.0000e+00])\n",
      "tensor(4.7451e-40, grad_fn=<AddBackward0>) 94 tensor([6.1613e-20, 0.0000e+00])\n",
      "tensor(1.7082e-40, grad_fn=<AddBackward0>) 95 tensor([3.6968e-20, 0.0000e+00])\n",
      "tensor(6.1497e-41, grad_fn=<AddBackward0>) 96 tensor([2.2181e-20, 0.0000e+00])\n",
      "tensor(2.2138e-41, grad_fn=<AddBackward0>) 97 tensor([1.3308e-20, 0.0000e+00])\n",
      "tensor(7.9706e-42, grad_fn=<AddBackward0>) 98 tensor([7.9850e-21, 0.0000e+00])\n",
      "tensor(2.8699e-42, grad_fn=<AddBackward0>) 99 tensor([4.7910e-21, 0.0000e+00])\n",
      "tensor(1.0342e-42, grad_fn=<AddBackward0>) 100 tensor([2.8746e-21, 0.0000e+00])\n",
      "tensor(3.7275e-43, grad_fn=<AddBackward0>) 101 tensor([1.7248e-21, 0.0000e+00])\n",
      "tensor(1.3452e-43, grad_fn=<AddBackward0>) 102 tensor([1.0349e-21, 0.0000e+00])\n",
      "tensor(4.7644e-44, grad_fn=<AddBackward0>) 103 tensor([6.2091e-22, 0.0000e+00])\n",
      "tensor(1.6816e-44, grad_fn=<AddBackward0>) 104 tensor([3.7255e-22, 0.0000e+00])\n",
      "tensor(5.6052e-45, grad_fn=<AddBackward0>) 105 tensor([2.2353e-22, 0.0000e+00])\n",
      "tensor(2.8026e-45, grad_fn=<AddBackward0>) 106 tensor([1.3412e-22, 0.0000e+00])\n",
      "tensor(0., grad_fn=<AddBackward0>) 107 tensor([8.0470e-23, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# 让 x 和 y 分别下降, 学习率为 0.1\n",
    "learning_rate = 0.1\n",
    "x = torch.Tensor([11, 2.0])\n",
    "x.requires_grad_(True)\n",
    "ys = []\n",
    "for i in range(1000):\n",
    "    y = 2*(x[0]**2)  + 4*x[1]**2\n",
    "    y.backward()\n",
    "    if  len(ys) and y >= ys[-1]:\n",
    "        break\n",
    "    ys.append(y)\n",
    "    print(y , i, x.grad )\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
