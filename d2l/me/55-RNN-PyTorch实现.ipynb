{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN PyTorch 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 构建训练集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['想', '要', '有', '直', '升', '机', ' ', '想', '要', '和', '你', '飞', '到', '宇', '宙', '去', ' ', '想', '要', '和', '你', '融', '化', '在', '一', '起', ' ', '融', '化', '在', '宇', '宙', '里', ' ', '我', '每', '天', '每', '天', '每', '天', '在', '想', '想', '想', '想', '著', '你', ' ', '这', '样', '的', '甜', '蜜', ' ', '让', '我', '开', '始', '乡', '相', '信', '命', '运', ' ', '感', '谢', '地', '心', '引', '力', ' ', '让', '我', '碰', '到', '你', ' ', '漂', '亮', '的', '让', '我', '面', '红', '的', '可', '爱', '女', '人', ' ', '温', '柔', '的', '让', '我', '心', '疼', '的', '可']\n"
     ]
    }
   ],
   "source": [
    "# 加载文本数据\n",
    "fp = os.path.abspath(\"./data/jaychou_lyrics.txt\")\n",
    "tokens = [char for char in \" \".join(open(fp, \"r\").readlines()).replace(\"\\n\", \"\")]\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:  \n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens): \n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2583"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 构建训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataLoader: \n",
    "    \"\"\"加载序列数据的迭代器\"\"\"\n",
    "    def __init__(self, tokens, batch_size, num_steps, use_random_iter=True, max_tokens=-1):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = self.seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = self.seq_data_iter_sequential\n",
    "        self.vocab = Vocab(tokens, min_freq=5)\n",
    "        self.corpus = [self.vocab[token] for token in tokens]\n",
    "        if max_tokens > 0:\n",
    "            self.corpus = self.corpus[:max_tokens]\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "    \n",
    "    def seq_data_iter_sequential(self, corpus, batch_size, num_steps):  #@save\n",
    "        \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "        # 从随机偏移量开始划分序列\n",
    "        offset = random.randint(0, num_steps)\n",
    "        num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "        Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "        Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "        Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "        num_batches = Xs.shape[1] // num_steps\n",
    "        for i in range(0, num_steps * num_batches, num_steps):\n",
    "            X = Xs[:, i: i + num_steps]\n",
    "            Y = Ys[:, i: i + num_steps]\n",
    "            yield X, Y\n",
    "\n",
    "    def seq_data_iter_random(self, corpus, batch_size, num_steps):  #@save\n",
    "        \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "        # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "        corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "        # 减去1，是因为我们需要考虑标签\n",
    "        num_subseqs = (len(corpus) - 1) // num_steps\n",
    "        # 长度为num_steps的子序列的起始索引\n",
    "        initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "        # 在随机抽样的迭代过程中，\n",
    "        # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "        random.shuffle(initial_indices)\n",
    "\n",
    "        def data(pos):\n",
    "            # 返回从pos位置开始的长度为num_steps的序列\n",
    "            return corpus[pos: pos + num_steps]\n",
    "\n",
    "        num_batches = num_subseqs // batch_size\n",
    "        for i in range(0, batch_size * num_batches, batch_size):\n",
    "            # 在这里，initial_indices包含子序列的随机起始索引\n",
    "            initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "            X = [data(j) for j in initial_indices_per_batch]\n",
    "            Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "            yield torch.tensor(X), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data_iter = SeqDataLoader(tokens, 3, 10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq_data_iter.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_data_iter.num_steps, seq_data_iter.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10]) torch.Size([3, 10])\n",
      "X:\n",
      "['想', '要', '有', '直', '升', '机', ' ', '想', '要', '和']\n",
      "['书', '框', '的', '城', '堡', '\\u3000', '像', '欧', '<unk>', '情']\n",
      "[' ', '会', '有', '两', '块', '空', '地', '那', '就', '是']\n",
      "Y:\n",
      "['要', '有', '直', '升', '机', ' ', '想', '要', '和', '你']\n",
      "['框', '的', '城', '堡', '\\u3000', '像', '欧', '<unk>', '情', '调']\n",
      "['会', '有', '两', '块', '空', '地', '那', '就', '是', '勇']\n",
      "torch.Size([3, 10]) torch.Size([3, 10])\n",
      "X:\n",
      "['你', '飞', '到', '宇', '宙', '去', ' ', '想', '要', '和']\n",
      "['调', ' ', '书', '框', '的', '城', '堡', '\\u3000', '像', '欧']\n",
      "['勇', '气', '与', '<unk>', '力', ' ', '我', '要', '做', '音']\n",
      "Y:\n",
      "['飞', '到', '宇', '宙', '去', ' ', '想', '要', '和', '你']\n",
      "[' ', '书', '框', '的', '城', '堡', '\\u3000', '像', '欧', '<unk>']\n",
      "['气', '与', '<unk>', '力', ' ', '我', '要', '做', '音', '乐']\n",
      "torch.Size([3, 10]) torch.Size([3, 10])\n",
      "X:\n",
      "['你', '融', '化', '在', '一', '起', ' ', '融', '化', '在']\n",
      "['<unk>', '情', '调', ' ', '对', '着', '这', '张', '海', '报']\n",
      "['乐', '上', '的', '<unk>', '<unk>', ' ', '哼', '哼', '哈', '兮']\n",
      "Y:\n",
      "['融', '化', '在', '一', '起', ' ', '融', '化', '在', '宇']\n",
      "['情', '调', ' ', '对', '着', '这', '张', '海', '报', '\\u3000']\n",
      "['上', '的', '<unk>', '<unk>', ' ', '哼', '哼', '哈', '兮', ' ']\n"
     ]
    }
   ],
   "source": [
    "for i, (X, Y) in enumerate(seq_data_iter):\n",
    "    if i > 2:\n",
    "        break\n",
    "    print(X.shape, Y.shape)\n",
    "    print(f\"X:\")\n",
    "    for x in X:\n",
    "        print(seq_data_iter.vocab.to_tokens(x.tolist()))\n",
    "    print(\"Y:\")\n",
    "    for y in Y:\n",
    "        print(seq_data_iter.vocab.to_tokens(y.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer    # RNN层\n",
    "        self.vocab_size = vocab_size    # 分词词库大小\n",
    "        self.num_hiddens = self.rnn.hidden_size     # 隐藏层的节点数\n",
    "        if not self.rnn.bidirectional:  # 如果是双向\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)  # 线性层 /输出层\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "    \n",
    "    def forward(self, inputs, state):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        :params: inputs: 输入\n",
    "        :params: state: 隐藏状态\n",
    "        \"\"\"\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "    \n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return torch.zeros((self.num_directions * self.rnn.num_layers, \n",
    "                                batch_size, self.num_hiddens),\n",
    "                                device = device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                     torch.zeros((\n",
    "                         self.num_directions * self.rnn.num_layers,\n",
    "                         batch_size, self.num_hiddens), device=device))#(h_n, c_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device):\n",
    "    \"\"\"在`prefix`后面生成新字符\"\"\"\n",
    "    #生成初始隐藏状态\n",
    "    state = net.begin_state(batch_size=1, device=device) \n",
    "    outputs = [vocab[prefix[0]]] #第一个word的整型下标\n",
    "    #将最近预测的词做成tensor, batch_size=1, n_step=1\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]: # 预热操作, 保存真值\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds): # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):#如果使用nn.Module来实现\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum(\n",
    "        (p.grad**2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch8(net, train_iter, loss, updater, device,\n",
    "                   use_random_iter):\n",
    "    \"\"\"训练模型一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:#为第一个batch 或者 batch之间时序上不连续\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device) #初始化state\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个张量\n",
    "                state.detach_() # 对之前的部分取消梯度反向传播计算\n",
    "            else:\n",
    "                # state对于nn.LSTM或者对于我们从零开始实现的模型是个元组(张量构成)\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1) #reshape真值, 将n_step放在第一维之后拉成一维向量\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat,state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):#调用torch优化函数实现\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                           legend=['train'], xlim=[10,num_epochs])\n",
    "    #初始化优化器\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    #训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch+1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 512])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 5\n",
    "num_steps = 10\n",
    "num_hiddens = 512\n",
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape #(D * num_layers(=1), batch_size, num_hiddens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_layer = nn.RNN(input_size=len(vocab), hidden_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data_iter = SeqDataLoader(tokens, batch_size, num_steps, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有冒膏柔阶丧膏膏膏膏膏'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = d2l.try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)\n",
    "predict_ch8('想要有', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, seq_data_iter, vocab, lr, num_epochs, device, use_random_iter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
