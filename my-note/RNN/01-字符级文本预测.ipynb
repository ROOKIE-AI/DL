{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字符级文本预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将使用一句简单的英语句子，例如 \"hello\"。目标是通过训练一个RNN模型，输入前几个字符后预测下一个字符。\n",
    "<br>\n",
    "步骤：\n",
    "1. 准备数据\n",
    "2. 定义RNN模型\n",
    "3. 训练模型\n",
    "4. 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据\n",
    "text = \"hello\"\n",
    "chars = sorted(list(set(text)))  # 获取字符集\n",
    "char_to_idx = {char: i for i, char in enumerate(chars)}\n",
    "idx_to_char = {i: char for i, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本转化为数字表示\n",
    "input_seq = [char_to_idx[c] for c in text[:-1]]  # \"hell\"\n",
    "target_seq = [char_to_idx[c] for c in text[1:]]  # \"ello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换为Tensor\n",
    "input_seq = torch.tensor(input_seq).unsqueeze(0)  # 加上batch维度\n",
    "target_seq = torch.tensor(target_seq).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.定义RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)   # 嵌入层\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)    # \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "vocab_size = len(chars)     # 词汇表的大小，即字符集的大小\n",
    "embed_size = 10            # 嵌入层的维度。每个字符将被映射为一个长度为 10 的向量。这是嵌入层的输出维度。\n",
    "hidden_size = 20           # RNN 隐藏层的维度。RNN 的隐藏状态（hidden state）将是一个长度为 20 的向量。\n",
    "output_size = vocab_size   # 输出层的维度。因为我们要预测下一个字符，输出层的大小应该等于词汇表的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "model = SimpleRNN(vocab_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.3334\n",
      "Epoch [20/100], Loss: 0.0502\n",
      "Epoch [30/100], Loss: 0.0154\n",
      "Epoch [40/100], Loss: 0.0076\n",
      "Epoch [50/100], Loss: 0.0050\n",
      "Epoch [60/100], Loss: 0.0039\n",
      "Epoch [70/100], Loss: 0.0032\n",
      "Epoch [80/100], Loss: 0.0028\n",
      "Epoch [90/100], Loss: 0.0025\n",
      "Epoch [100/100], Loss: 0.0022\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = None  # 初始隐藏状态\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(input_seq, hidden)\n",
    "    \n",
    "    loss = criterion(output.view(-1, vocab_size), target_seq.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: h, Predicted: e\n",
      "Input: e, Predicted: l\n",
      "Input: l, Predicted: l\n",
      "Input: l, Predicted: o\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "with torch.no_grad():\n",
    "    hidden = None\n",
    "    input_char = 'h'\n",
    "    input_idx = torch.tensor([[char_to_idx[input_char]]])\n",
    "    \n",
    "    for _ in range(len(text) - 1):\n",
    "        output, hidden = model(input_idx, hidden)\n",
    "        _, predicted_idx = torch.max(output, 2)\n",
    "        predicted_char = idx_to_char[predicted_idx.item()]\n",
    "        print(f'Input: {input_char}, Predicted: {predicted_char}')\n",
    "        \n",
    "        input_char = predicted_char\n",
    "        input_idx = torch.tensor([[char_to_idx[input_char]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
