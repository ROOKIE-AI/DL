{"metadata":{"jupytext":{"formats":"md,ipynb","split_at_heading":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1226004,"sourceType":"datasetVersion","datasetId":701370},{"sourceId":1338830,"sourceType":"datasetVersion","datasetId":708136},{"sourceId":1363948,"sourceType":"datasetVersion","datasetId":701538},{"sourceId":1495782,"sourceType":"datasetVersion","datasetId":878523},{"sourceId":1513167,"sourceType":"datasetVersion","datasetId":617086}],"dockerImageVersionId":30648,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Computer Vision](https://www.kaggle.com/learn/computer-vision) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/data-augmentation).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn these exercises, you'll explore what effect various random transformations have on an image, consider what kind of augmentation might be appropriate on a given dataset, and then use data augmentation with the *Car or Truck* dataset to train a custom network.\n\nRun the cell below to set everything up!","metadata":{}},{"cell_type":"code","source":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.computer_vision.ex6 import *\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n# Imports\nimport os, warnings\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Reproducability\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    #os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\") # to clean up output cells\n\n\n# Load training and validation sets\nds_train_ = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=True,\n)\nds_valid_ = image_dataset_from_directory(\n    '../input/car-or-truck/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=64,\n    shuffle=False,\n)\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nds_train = (\n    ds_train_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\nds_valid = (\n    ds_valid_\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=AUTOTUNE)\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (Optional) Explore Augmentation #\n\nUncomment a transformation and run the cell to see what it does. You can experiment with the parameter values too, if you like. (The `factor` parameters should be greater than 0 and, generally, less than 1.) Run the cell again if you'd like to get a new random image.","metadata":{}},{"cell_type":"code","source":"# all of the \"factor\" parameters indicate a percent-change\naugment = keras.Sequential([\n    # preprocessing.RandomContrast(factor=0.5),\n    preprocessing.RandomFlip(mode='horizontal'), # meaning, left-to-right\n    # preprocessing.RandomFlip(mode='vertical'), # meaning, top-to-bottom\n    # preprocessing.RandomWidth(factor=0.15), # horizontal stretch\n    # preprocessing.RandomRotation(factor=0.20),\n    # preprocessing.RandomTranslation(height_factor=0.1, width_factor=0.1),\n])\n\n\nex = next(iter(ds_train.unbatch().map(lambda x, y: x).batch(1)))\n\nplt.figure(figsize=(10,10))\nfor i in range(16):\n    image = augment(ex, training=True)\n    plt.subplot(4, 4, i+1)\n    plt.imshow(tf.squeeze(image))\n    plt.axis('off')\nplt.show()","metadata":{"lines_to_next_cell":0},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do the transformations you chose seem reasonable for the *Car or Truck* dataset?","metadata":{}},{"cell_type":"markdown","source":"In this exercise, we'll look at a few datasets and think about what kind of augmentation might be appropriate. Your reasoning might be different that what we discuss in the solution. That's okay. The point of these problems is just to think about how a transformation might interact with a classification problem -- for better or worse.","metadata":{}},{"cell_type":"markdown","source":"The [EuroSAT](https://www.kaggle.com/ryanholbrook/eurosat) dataset consists of satellite images of the Earth classified by geographic feature. Below are a number of images from this dataset.\n\n<figure>\n<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/LxARYZe.png\" width=600, alt=\"Sixteen satellite images labeled: SeaLake, PermanentCrop, Industrial, Pasture, Residential, and Forest.\">\n</figure>","metadata":{}},{"cell_type":"markdown","source":"# 1) EuroSAT #\n\nWhat kinds of transformations might be appropriate for this dataset?","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this code cell to receive credit!)\nq_1.check()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint \n#q_1.solution()","metadata":{"lines_to_next_cell":2},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The [TensorFlow Flowers](https://www.kaggle.com/ryanholbrook/tensorflow-flowers) dataset consists of photographs of flowers of several species. Below is a sample.\n\n<figure>\n<img src=\"https://storage.googleapis.com/kaggle-media/learn/images/Mt7PR2x.png\" width=600, alt=\"Sixteen images of flowers labeled: roses, tulips, dandelion, and sunflowers\">\n</figure>","metadata":{}},{"cell_type":"markdown","source":"# 2) TensorFlow Flowers #\n\nWhat kinds of transformations might be appropriate for the TensorFlow Flowers dataset?","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this code cell to receive credit!)\nq_2.check()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint \n#q_2.solution()","metadata":{"lines_to_next_cell":2},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you'll use data augmentation with a custom convnet similar to the one you built in Exercise 5. Since data augmentation effectively increases the size of the dataset, we can increase the capacity of the model in turn without as much risk of overfitting.","metadata":{}},{"cell_type":"markdown","source":"# 3) Add Preprocessing Layers #\n\nAdd these preprocessing layers to the given model.\n\n```\npreprocessing.RandomContrast(factor=0.10),\npreprocessing.RandomFlip(mode='horizontal'),\npreprocessing.RandomRotation(factor=0.10),\n```\n","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.InputLayer(input_shape=[128, 128, 3]),\n    \n    # Data Augmentation\n    # ____,\n\n    # Block One\n    layers.BatchNormalization(renorm=True),\n    layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Block Two\n    layers.BatchNormalization(renorm=True),\n    layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Block Three\n    layers.BatchNormalization(renorm=True),\n    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n    layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same'),\n    layers.MaxPool2D(),\n\n    # Head\n    layers.BatchNormalization(renorm=True),\n    layers.Flatten(),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(1, activation='sigmoid'),\n])\n\n# Check your answer\nq_3.check()","metadata":{"lines_to_next_cell":2},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_3.hint()\n#q_3.solution()","metadata":{"lines_to_next_cell":2},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll train the model. Run the next cell to compile it with a loss and accuracy metric and fit it to the training set.","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(epsilon=0.01)\nmodel.compile(\n    optimizer=optimizer,\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n\nhistory = model.fit(\n    ds_train,\n    validation_data=ds_valid,\n    epochs=50,\n)\n\n# Plot learning curves\nimport pandas as pd\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['loss', 'val_loss']].plot()\nhistory_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();","metadata":{"lines_to_next_cell":2},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) Train Model #\n\nExamine the training curves. What there any sign of overfitting? How does the performance of this model compare to other models you've trained in this course?","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this code cell to receive credit!)\nq_4.solution()","metadata":{"lines_to_next_cell":0},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion #\n\nData augmentation is a powerful and commonly-used tool to improve model training, not only for convolutional networks, but for many other kinds of neural network models as well. Whatever your problem, the principle remains the same: you can make up for an inadequacy in your data by adding in \"fake\" data to cover it over. Experimenting with augmentations is a great way to find out just how far your data can go!","metadata":{}},{"cell_type":"markdown","source":"# The End #\n\nThat's all for **Computer Vision** on Kaggle Learn! Are you ready to apply your knowledge? Check out our two bonus lessons! They'll walk you through preparing a submission for a competition while you learn how to train neural nets with TPUs, Kaggle's most advanced accelerator. At the end, you'll have a complete notebook ready to extend with ideas of your own.\n- [Create Your First Submission](https://www.kaggle.com/ryanholbrook/create-your-first-submission) - Prepare a submission for our *Petals to the Metal* Getting Started competition. You'll train a neural net to recognize over 100 species of flowers.\n- [Cassava Leaf Disease](https://www.kaggle.com/jessemostipak/getting-started-tpus-cassava-leaf-disease) - Rather compete for money and medals? Train a neural net to diagnose common diseases in the cassava plant, a staple security crop in Africa.\n\nHave fun learning!","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/computer-vision/discussion) to chat with other learners.*","metadata":{}}]}